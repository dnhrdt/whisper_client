Directory structure:
└── dimastatz-whisper-flow/
    ├── README.md
    ├── Dockerfile.test
    ├── LICENSE
    ├── requirements.txt
    ├── run.sh
    ├── setup.py
    ├── docs/
    │   ├── demo1.md
    │   ├── metavox.md
    │   ├── monetizations.md
    │   ├── plans.md
    │   └── imgs/
    │       └── charts.drawio
    ├── tests/
    │   ├── __init__.py
    │   ├── test_chat_room.py
    │   ├── test_streaming.py
    │   ├── test_transcriber.py
    │   ├── utils.py
    │   ├── audio/
    │   │   ├── __init__.py
    │   │   └── test_audio.py
    │   ├── benchmark/
    │   │   ├── __init__.py
    │   │   └── test_benchmark.py
    │   ├── examples/
    │   │   └── mic_transcribe.py
    │   └── resources/
    │       └── 3081-166546-0000.json
    ├── whisperflow/
    │   ├── __init__.py
    │   ├── chat_room.py
    │   ├── fast_server.py
    │   ├── streaming.py
    │   ├── transcriber.py
    │   ├── audio/
    │   │   └── microphone.py
    │   └── models/
    │       └── tiny.en.pt
    ├── .devcontainer/
    │   └── devcontainer.json
    └── .github/
        ├── dependabot.yml
        └── workflows/
            └── docker-image.yml

================================================
File: README.md
================================================
<div align="center">
<h1 align="center"> Whisper Flow </h1>
<h3>Real-Time Transcription Using OpenAI Whisper</br></h3>
<img src="https://img.shields.io/badge/Progress-100%25-red"> <img src="https://img.shields.io/badge/Feedback-Welcome-green">
</br>
</br>
<kbd>
<img src="https://github.com/dimastatz/whisper-flow/blob/da8b67c6180566b987854b2fb94670fee92e6682/docs/imgs/whisper-flow.png?raw=true" width="256px">
</kbd>
</div>

## About The Project

### OpenAI Whisper
OpenAI [Whisper](https://github.com/openai/whisper) is a versatile speech recognition model designed for general use. Trained on a vast and varied audio dataset, Whisper can handle tasks such as multilingual speech recognition, speech translation, and language identification. It is commonly used for batch transcription, where you provide the entire audio or video file to Whisper, which then converts the speech into text. This process is not done in real-time; instead, Whisper processes the files and returns the text afterward, similar to handing over a recording and receiving the transcript later.

### Whisper Flow
Using Whisper Flow, you can generate real-time transcriptions for your media content. Unlike batch transcriptions, where media files are uploaded and processed, streaming media is delivered to Whisper Flow in real time, and the service returns a transcript immediately.

### What is Streaming
Streaming content is sent as a series of sequential data packets, or 'chunks,' which Whisper Flow transcribes on the spot. The benefits of using streaming over batch processing include the ability to incorporate real-time speech-to-text functionality into your applications and achieving faster transcription times. However, this speed may come at the expense of accuracy in some cases.

### Stream Windowing
In scenarios involving time-streaming, it's typical to perform operations on data within specific time frames known as temporal windows. One common approach is using the [tumbling window](https://learn.microsoft.com/en-us/azure/stream-analytics/stream-analytics-window-functions#tumbling-window) technique, which involves gathering events into segments until a certain condition is met.

<div align="center">
<img src="https://github.com/dimastatz/whisper-flow/blob/main/docs/imgs/streaming.png?raw=true">
<div>Tumbling Window</div>
</div><br/>

### Streaming Results
Whisper Flow splits the audio stream into segments based on natural speech patterns, like speaker changes or pauses. The transcription is sent back as a series of events, with each response containing more transcribed speech until the entire segment is complete.

| Transcript                                    | EndTime  | IsPartial |
| :-------------------------------------------- | :------: | --------: |
| Reality                                       |   0.55   | True      |
| Reality is created                            |   1.05   | True      |
| Reality is created by the                     |   1.50   | True      |
| Reality is created by the mind                |   2.15   | True      |
| Reality is created by the mind                |   2.65   | False     |
| we can                                        |   3.05   | True      |
| we can change                                 |   3.45   | True      |
| we can change reality                         |   4.05   | True      |
| we can change reality by changing             |   4.45   | True      |
| we can change reality by changing our mind    |   5.05   | True      |
| we can change reality by changing our mind    |   5.55   | False     |

### Benchmarking
The evaluation metrics for comparing the performance of Whisper Flow are Word Error Rate (WER) and latency. Latency is measured as the time between two subsequent partial results, with the goal of achieving sub-second latency. We're not starting from scratch, as several quality benchmarks have already been performed for different ASR engines. I will rely on the research article ["Benchmarking Open Source and Paid Services for Speech to Text"](https://www.frontiersin.org/articles/10.3389/fdata.2023.1210559/full) for guidance. For benchmarking the current implementation of Whisper Flow, I use [LibriSpeech](https://www.openslr.org/12).

```bash
| Partial | Latency | Result |

True  175.47  when we took
True  185.14  When we took her.
True  237.83  when we took our seat.
True  176.42  when we took our seats.
True  198.59  when we took our seats at the
True  186.72  when we took our seats at the
True  210.04  when we took our seats at the breakfast.
True  220.36  when we took our seats at the breakfast table.
True  203.46  when we took our seats at the breakfast table.
True  242.63  When we took our seats at the breakfast table, it will
True  237.41  When we took our seats at the breakfast table, it was with
True  246.36  When we took our seats at the breakfast table, it was with the
True  278.96  When we took our seats at the breakfast table, it was with the feeling.
True  285.03  When we took our seats at the breakfast table, it was with the feeling of being.
True  295.39  When we took our seats at the breakfast table, it was with the feeling of being no
True  270.88  When we took our seats at the breakfast table, it was with the feeling of being no longer
True  320.43  When we took our seats at the breakfast table, it was with the feeling of being no longer looked
True  303.66  When we took our seats at the breakfast table, it was with the feeling of being no longer looked upon.
True  470.73  When we took our seats at the breakfast table, it was with the feeling of being no longer
True  353.25  When we took our seats at the breakfast table, it was with the feeling of being no longer looked upon as connected.
True  345.74  When we took our seats at the breakfast table, it was with the feeling of being no longer looked upon as connected in any way.
True  368.66  When we took our seats at the breakfast table, it was with the feeling of being no longer looked upon as connected in any way with the
True  400.25  When we took our seats at the breakfast table, it was with the feeling of being no longer looked upon as connected in any way with this case.
True  382.71  When we took our seats at the breakfast table, it was with the feeling of being no longer looked upon as connected in any way with this case.
False 405.02  When we took our seats at the breakfast table, it was with the feeling of being no longer looked upon as connected in any way with this case.
```

When running this benchmark on a MacBook Air with an [M1 chip and 16GB of RAM](https://support.apple.com/en-il/111883#:~:text=Testing%20conducted%20by%20Apple%20in,to%208%20clicks%20from%20bottom.), we achieve impressive performance metrics. The latency is consistently well below 500ms, ensuring real-time responsiveness. Additionally, the word error rate is around 7%, demonstrating the accuracy of the transcription.

```bash
Latency Stats:
count     26.000000
mean     275.223077
std       84.525695
min      154.700000
25%      205.105000
50%      258.620000
75%      339.412500
max      470.700000
```

### How To Use it

#### As a Web Server
To run WhisperFlow as a web server, start by cloning the repository to your local machine.
```bash
git clone https://github.com/dimastatz/whisper-flow.git
```
Then navigate to WhisperFlow folder, create a local venv with all dependencies and run the web server on port 8181.
```bash
cd whisper-flow
./run.sh -local
source .venv/bin/activate
./run.sh -benchmark
```

#### As a Python Package
Set up a WebSocket endpoint for real-time transcription by retrieving the transcription model and creating asynchronous functions for transcribing audio chunks and sending JSON responses. Manage the WebSocket connection by continuously processing incoming audio data. Handle terminate exception to stop the session and close the connection if needed.

Start with installing whisper python package

```bash
pip install whisperflow
```

Now import whsiperflow and transcriber modules

```Python
import whisperflow.streaming as st
import whisperflow.transcriber as ts

@app.websocket("/ws")
async def websocket_endpoint(websocket: WebSocket):
    model = ts.get_model()

    async def transcribe_async(chunks: list):
        return await ts.transcribe_pcm_chunks_async(model, chunks)

    async def send_back_async(data: dict):
        await websocket.send_json(data)

    try:
        await websocket.accept()
        session = st.TrancribeSession(transcribe_async, send_back_async)

        while True:
            data = await websocket.receive_bytes()
            session.add_chunk(data)
    except Exception as exception:
        await session.stop()
        await websocket.close()
```
#### Roadmap
- [X] Release v1.0-RC - Includes transcription streaming implementation.
- [X] Release v1.1 - Bug fixes and implementation of the most requested changes.
- [ ] Release v1.2 - Prepare the package for integration with the py-speech package.


================================================
File: Dockerfile.test
================================================
FROM python:3.11-slim-buster

# Install system dependencies
RUN apt-get update && apt-get install -y \
    portaudio19-dev \
    python3-dev \
    build-essential \
    && rm -rf /var/lib/apt/lists/*

# Set the working directory
WORKDIR /app

# Copy requirements file
COPY requirements.txt .

# Install dependencies
RUN pip install --no-cache-dir -r requirements.txt

# Copy the rest of the application
COPY . .

RUN rm -rf .venv & python3 -m venv .venv & source .venv/bin/activate \
    & pip install --upgrade pip & pip install -r ./requirements.txt \
    & black whisperflow tests & pylint --fail-under=9.9 whisperflow tests \
    & pytest --ignore=tests/benchmark --ignore=tests/audio --cov-fail-under=95 --cov whisperflow -v tests


================================================
File: LICENSE
================================================
MIT License

Copyright (c) 2024 Dima Statz

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
SOFTWARE.


================================================
File: requirements.txt
================================================
jiwer==3.0.4
pytest==7.3.2
black==23.3.0
pandas==2.2.2
httpx==0.27.0
pylint==2.17.4
PyAudio==0.2.14
fastapi==0.108.0
pytest-cov==4.1.0
pytest-timeout==2.3.1
pytest-asyncio==0.23.7
pytest-benchmark==4.0.0
websocket-client==1.8.0
python-multipart==0.0.9
openai-whisper==20231117
pylint-fail-under==0.3.0
uvicorn[standard]==0.30.1


================================================
File: run.sh
================================================
#!/bin/bash

abort()
{
    echo "*** FAILED ***" >&2
    exit 1
}

if [ "$#" -eq 0 ]; then
    echo "No arguments provided. Usage:
    1. '-local' to build local environment
    2. '-docker' to build and run docker container
    3. '-test' to run linter, formatter and tests
    4. '-benchmark' to run benchmark tests
    5. '-run-server' to run fastapi server
    6. '-setup' to run package setup"
elif [ $1 = "-local" ]; then
    trap 'abort' 0
    set -e
    echo "Running format, linter and tests"
    rm -rf .venv
    python3 -m venv .venv
    source .venv/bin/activate
    pip install --upgrade pip
    pip install -r ./requirements.txt

    black whisperflow tests
    pylint --fail-under=9.9 whisperflow tests
    pytest --ignore=tests/benchmark --cov-fail-under=95 --cov whisperflow -v tests
elif [ $1 = "-test" ]; then
    trap 'abort' 0
    set -e

    echo "Running format, linter and tests"
    source .venv/bin/activate
    black whisperflow tests
    pylint --fail-under=9.9 whisperflow tests
    pytest --ignore=tests/benchmark --cov-fail-under=95 --cov --log-cli-level=INFO whisperflow -v tests
elif [ $1 = "-docker" ]; then
    echo "Building and running docker image"
    docker stop whisperflow-container
    docker rm whisperflow-container
    docker rmi whisperflow-image
    # build docker and run
    docker build --tag whisperflow-image --build-arg CACHEBUST=$(date +%s) . --file Dockerfile.test
    docker run --name whisperflow-container -p 8888:8888 -d whisperflow-image
elif [ $1 = "-benchmark" ]; then
    echo "Running WhisperFlow Server"
    kill $(lsof -t -i:8181)
    nohup uvicorn whisperflow.fast_server:app --host 0.0.0.0 --port 8181 &
    sleep 2s
    echo "Running WhisperFlow benchmark tests"
    pytest -v -s tests/benchmark
    kill $(lsof -t -i:8181)
elif [ $1 = "-run-server" ]; then
    echo "Running WhisperFlow server"
    kill $(lsof -t -i:8181)
    uvicorn whisperflow.fast_server:app --host 0.0.0.0 --port 8181
elif [ $1 = "-test-package" ]; then
    echo "Running WhisperFlow package setup"
    # pip install twine
    # pip install wheel
    python setup.py sdist bdist_wheel
    rm -rf .venv_test
    python3 -m venv .venv_test
    source .venv_test/bin/activate
    pip install ./dist/whisperflow-0.1.0-py3-none-any.whl
    pytest --ignore=tests/benchmark --cov-fail-under=95 --cov whisperflow -v tests
    # twine upload ./dist/*
else
  echo "Wrong argument is provided. Usage:
    1. '-local' to build local environment
    2. '-docker' to build and run docker container
    3. '-test' to run linter, formatter and tests
    4. '-benchmark' to run benchmark tests
    5. '-run-server' to run fastapi server
    6. '-setup' to run package setup"
fi

trap : 0
echo >&2 '*** DONE ***'

================================================
File: setup.py
================================================
from pathlib import Path
from setuptools import setup
from whisperflow import __version__
from pkg_resources import parse_requirements


this_directory = Path(__file__).parent
long_description = (this_directory / "README.md").read_text()

setup(
    name='whisperflow',
    version=__version__,
    url='https://github.com/dimastatz/whisper-flow',
    author='Dima Statz',
    author_email='dima.statz@gmail.com',
    py_modules=['whisperflow'],
    python_requires=">=3.8",
    install_requires=[
        str(r)
        for r in parse_requirements(
            Path(__file__).with_name("requirements.txt").open()
        )
    ],
    description='WhisperFlow: Real-Time Transcription Powered by OpenAI Whisper',
    long_description = long_description,
    long_description_content_type='text/markdown',
    include_package_data=True,
    package_data={'': ['static/*']},
)


================================================
File: docs/demo1.md
================================================

# Demo 1: Performance Benchmark of WHisper-Flow

## Prompt:

### Slide 1: Introduction to Whisper-Flow Demo
"Welcome, everyone! In this demo, I’m going to show you how Whisper-Flow works and highlight its performance, particularly focusing on the latency of partial and non-partial transcription results."

### Slide 2: Cloning the Repository
"First, let’s start by cloning the Whisper-Flow repository from GitHub. This is a simple step, and here’s the command I’ll use:"

```bash
git clone https://github.com/dimastatz/whisper-flow.git
```
"This gives us access to all the necessary files to run Whisper-Flow locally."

### Slide 3: Setting Up the Local Environment
"Now that we have the repository, I’m going to set up the local environment. Here’s what I’ll do:"
```bash
Copy code
cd whisper-flow
./run.sh -local
source .venv/bin/activate
```

"This creates a virtual environment and installs all dependencies. We’re almost ready to run the server."

### Slide 4: Starting the Whisper-Flow Server
"Next, I’m going to start the Whisper-Flow server. It will run locally on my machine, listening for requests on port 8181. Here’s the command:"
```bash
Copy code
uvicorn whisperflow.fast_server:app --host 0.0.0.0 --port 8181
```

"Now that the server is running, we’re ready to capture audio from my microphone and transcribe it in real-time."

### Slide 5: Real-Time Transcription in Action
"Now, let’s see Whisper-Flow in action. I’m going to run a Python script that captures audio from my local microphone. As I speak, Whisper-Flow will transcribe my speech in real-time, and you’ll see both partial and non-partial results appearing on the screen."
"Here’s the sentence I’m going to speak:"
"Now please pay attention, as I speak, Whisper-Flow transcribes text in real-time. You will see both partial and non-partial results appearing on the screen."
[Pause here to begin speaking and show the transcription on the screen]
"Notice how the partial results start appearing as I speak. These partial results give immediate feedback on what the system predicts, even before I’ve finished my sentence."
"Once I finish speaking, you’ll see the non-partial results. These are the final transcriptions, where Whisper-Flow refines and confirms the text for higher accuracy."
[Allow time for the transcription to finalize and explain further]
"This demonstrates the low latency and high performance of Whisper-Flow, which handles real-time speech processing with very little delay. Partial results show up instantly, and non-partial results provide the finalized, more accurate transcription once the input is complete."

### Slide 6: Conclusion
"So, in this demo, we’ve seen how Whisper-Flow transcribes audio in real-time, showcasing both partial and non-partial results with minimal latency. This capability is key for use cases where immediate feedback and accuracy are critical, such as live captioning or voice-assisted applications."
"Thank you for your attention, and I’m happy to answer any questions!"


================================================
File: docs/metavox.md
================================================
# Metavox Narrate Video Content in Different Languages

## Problem Statement
Content creators, such as YouTube bloggers, face challenges in expanding their reach internationally due to language barriers. While subtitles are commonly used, they don’t offer the immersive experience that narration in the viewer's native language provides. Many creators lack the resources or technical expertise to dub their content, limiting their ability to engage non-native audiences. For creators seeking to grow their international audience, an accessible, automated solution to translate and narrate videos in different languages is essential.

## Solution Overview
Metavox is a service that leverages advanced AI and video processing technologies to automate multilingual video narration. It combines Speech-to-Text (STT), Text-to-Speech (TTS), Translation, and Video Editing in one seamless workflow. Metavox can take videos directly from YouTube or accept local files and produce high-quality narrations in target languages. The process is straightforward:
- Extract and Transcribe Audio: STT technology converts the video’s audio into text in the original language.
- Translate Text: Translation APIs then convert this text to the desired target language(s).
- Generate Narration: TTS synthesizes a natural-sounding narration in the target language.
- Seamlessly Edit Video: Metavox combines the new audio with the original visuals, ensuring synchronized lip movement where possible and maintaining visual quality.

With this automated approach, creators can effortlessly produce an exact duplicate of their video content in multiple languages, enhancing accessibility and expanding their global reach.

## Expected Impact
Metavox is poised to offer transformative benefits for content creators:

- Audience Growth: Expands creators’ reach to non-native speaking audiences by providing an engaging, localized viewing experience.
- Increased Engagement: Boosts viewer retention and engagement by delivering content in the audience’s native language.
- Cost-Effective: Reduces or eliminates the need for manual dubbing, saving creators time and money.
- Scalability: Allows creators to duplicate videos across multiple languages quickly and easily.

By addressing these needs, Metavox can become the go-to solution for creators aiming to make their content universally accessible.

## Challenges and Assumptions
- Technical Complexity: Ensuring accurate and synchronized narration for complex languages and dialects may present challenges.
- Voice Quality and Emotion: Retaining the original tone, emotion, and style through synthetic voices might require custom TTS models.
- Copyright and Licensing: Adhering to platform-specific licensing rules (e.g., YouTube’s API policies) is essential to avoid copyright issues.

## Conclusion
Metavox empowers content creators to unlock new audiences and amplify their content's impact by removing language as a barrier. By streamlining multilingual narration, Metavox represents a leap forward in video localization technology, making global content creation both accessible and scalable.

================================================
File: docs/monetizations.md
================================================
# Monetization

## Known Strategies
Monetizing an open-source Python library is possible through a variety of strategies that balance openness with ways to generate revenue. Here are some effective ways to do it.

- Offer Paid Support and Consulting:
Provide technical support packages, priority support, or consulting services for organizations using your library. Many companies are willing to pay for timely, expert support and help with integration or customization.

- Dual Licensing Model:
Release the library under a permissive open-source license for personal and educational use, while offering a commercial license for businesses. This allows companies to use the library in proprietary projects, typically for a fee.

- Freemium Model with Premium Features:
Offer the core features of your library as open-source, and add advanced, enterprise-grade features under a paid model. Premium features could include extra integrations, performance optimizations, or unique modules that enhance the base library.

- SaaS Offering Based on the Library:
Create a Software-as-a-Service (SaaS) around your library. For instance, if your library performs data processing or analysis, you could build a cloud service that handles the setup, scaling, and maintenance, while users access it via an API.

- Sponsored Development:
Collaborate with companies that benefit from your library and offer them sponsorship opportunities. Sponsors can fund new feature development, security audits, or even be featured on the project's website or documentation as contributors.

- Donations and Crowdfunding:
Use platforms like Patreon, Open Collective, or GitHub Sponsors to receive recurring contributions. For significant new features, consider a crowdfunding approach where users contribute specifically to fund development.

- Sell Courses, Workshops, and Certification Programs:
Offer training materials, tutorials, or workshops that teach users how to maximize the library’s utility. A certification program can be a good option if your library is complex enough to require specialized knowledge.

- Customized and Extended Versions for Enterprises:
Offer customized solutions for enterprises that need specialized versions of your library. Tailoring your library to fit specific enterprise needs can justify a substantial fee.

- Branded Merchandise or Community-Driven Products:
If your library has a strong community, you might consider selling branded merchandise or offering a membership that comes with additional perks, like exclusive content, private webinars, or early access to new features.

Choosing the right monetization approach will depend on the library’s audience, the value it provides, and the community's needs. Combining a few of these methods can create sustainable revenue without compromising the library’s open-source values.


## For MetaVox

- Dual Licensing Model: We will release the library under an open-source license for personal and educational use while offering a paid commercial license for businesses. This enables companies to use the library in proprietary projects without open-source restrictions.

- Freemium Model with Premium Features: The core features will remain open-source, while advanced, enterprise-specific features such as extra integrations and performance optimizations will be available through a paid model.

- SaaS Offering: We will create a Software-as-a-Service (SaaS) around the library, allowing users to access its functionality through a cloud-hosted platform, with subscription-based pricing for scalable access and ease of use.

These strategies will allow us to maintain open access for the community while generating revenue from enterprises and SaaS users.


Is this submission being made to claim treaty benefits?

================================================
File: docs/plans.md
================================================
# Application ideas that can be built on top of WhisperFlow
Application ideas for existing businesses that could leverage WhisperFlow and large language models (LLMs) to automate up to 80% of tasks.

## Linga - an English level Assessment Application
Linga is a cutting-edge language learning platform designed to revolutionize language acquisition and standardized test preparation, specifically for TOEFL and IELTS. By utilizing WhisperFlow’s real-time transcription and advanced large language models (LLMs), Linga offers an interactive, engaging experience that can effectively replace traditional preparation courses.

Key Features:

- Real-Time Transcription: Instantly captures spoken interactions, allowing learners to engage fully in conversation practice.
- Personalized Feedback: Analyzes speech patterns and grammar, providing tailored feedback for skill refinement.
- Targeted Test Preparation: Includes modules focused on listening, speaking, and writing skills essential for TOEFL and IELTS success.
- Simulated Speaking Tests: Offers practice tests that mirror the format and timing of standardized exams, with immediate feedback.
- Progress Tracking: Monitors user performance, highlighting strengths and areas for improvement.

By combining automated conversation practice with real-time feedback tailored to standardized testing, Linga empowers learners to build the necessary skills for exam success. This innovative approach positions Linga as a viable alternative to traditional TOEFL and IELTS preparation courses, appealing to individual learners and educational institutions alike.


## LitMind - an interactive book reader Service.
LitMind is an interactive SaaS platform that transforms reading into an engaging, conversational experience. Users upload any book to LitMind, which then uses text-to-speech (TTS) technology to narrate it aloud. After each section, LitMind pauses to engage readers in a dynamic Q&A discussion, encouraging deeper understanding and reflection. LitMind’s AI-driven prompts ask thought-provoking questions and can answer user inquiries, simulating an interactive book club experience.

Key features include:

- Immersive Audio Narration: TTS technology narrates the book, allowing users to listen and focus on the story flow.
- Interactive Q&A: At natural breaks, LitMind invites users to discuss themes, characters, and ideas, sparking curiosity and deepening comprehension.
- AI-Driven Insights: LitMind’s conversational AI responds to reader questions and provides context, enhancing the depth of the reading experience.
- Personalized Engagement: Adapts questions and prompts based on the user’s responses, making each interaction unique and relevant.
- Multi-Device Compatibility: Accessible across all devices, perfect for on-the-go reading and learning.

LitMind is designed for readers seeking a more immersive, reflective experience with their books. Whether for personal growth, study, or enjoyment, LitMind’s interactive approach brings literature to life, creating an unparalleled digital reading journey.


## MetaVox - Live editor - movies narrator to any language
A SaaS platform that automatically translates and narrates YouTube videos into multiple languages for global reach. Users upload or link their original video, and the platform handles everything from audio translation to narration, encoding, and re-upload. For example, an Italian travel blogger could use the service to narrate their videos in English and German, making them accessible to a broader audience.

Key Features:

- Automated Translation and Voiceover – Supports multiple languages with natural-sounding AI narration.
- One-Click Encoding and Re-Upload – Processes the original video and re-uploads it in new languages.
- Localization Options – Customizes tone and dialect for target audiences.
- YouTube Integration – Links directly to YouTube for seamless workflow.
- Analytics – Engagement metrics per language for performance insights.

## SlideCrafter - Create slides and slides narration from a link or document
A new SAAS platform that instantly turns Google Slides and Microsoft PowerPoint presentations into narrated video clips. Ideal for professionals, educators, and content creators, this tool simplifies the process of creating engaging presentations by adding voice narration and exporting to video format with just a few clicks.

Key Features:

- Automatic Slide Narration: Simply type or speak your script, and the software will narrate each slide using natural-sounding, AI-powered voices.
- Video Clip Export: Instantly create high-quality, sharable video clips of your narrated presentations in multiple formats, making it ideal for remote and asynchronous communication.
- Built-In Editing Tools: Easily edit narration, timing, and transitions, giving users complete control over the final presentation without needing extensive video editing skills.
- Customizable Branding: Add logos, watermarks, and color schemes to match your brand, allowing for polished and professional presentations every time.



================================================
File: docs/imgs/charts.drawio
================================================
<mxfile host="65bd71144e">
    <diagram id="OqGMwIvQvN098w_085p2" name="Page-1">
        <mxGraphModel dx="918" dy="673" grid="1" gridSize="4" guides="1" tooltips="1" connect="1" arrows="1" fold="1" page="1" pageScale="1" pageWidth="1100" pageHeight="850" math="0" shadow="0">
            <root>
                <mxCell id="0"/>
                <mxCell id="1" parent="0"/>
                <mxCell id="3" value="" style="rounded=0;whiteSpace=wrap;html=1;fillColor=default;" vertex="1" parent="1">
                    <mxGeometry x="48" y="304" width="104" height="31" as="geometry"/>
                </mxCell>
                <mxCell id="2" value="" style="endArrow=classic;html=1;dashed=1;dashPattern=1 1;" edge="1" parent="1">
                    <mxGeometry width="50" height="50" relative="1" as="geometry">
                        <mxPoint x="48" y="352" as="sourcePoint"/>
                        <mxPoint x="496" y="352" as="targetPoint"/>
                    </mxGeometry>
                </mxCell>
                <mxCell id="4" value="" style="rounded=0;whiteSpace=wrap;html=1;fillColor=default;" vertex="1" parent="1">
                    <mxGeometry x="56" y="311.5" width="16" height="16" as="geometry"/>
                </mxCell>
                <mxCell id="5" value="" style="rounded=0;whiteSpace=wrap;html=1;fillColor=default;" vertex="1" parent="1">
                    <mxGeometry x="80" y="311.5" width="16" height="16" as="geometry"/>
                </mxCell>
                <mxCell id="6" value="" style="rounded=0;whiteSpace=wrap;html=1;fillColor=default;" vertex="1" parent="1">
                    <mxGeometry x="104" y="311.5" width="16" height="16" as="geometry"/>
                </mxCell>
                <mxCell id="7" value="" style="rounded=0;whiteSpace=wrap;html=1;fillColor=default;" vertex="1" parent="1">
                    <mxGeometry x="128" y="311.5" width="16" height="16" as="geometry"/>
                </mxCell>
                <mxCell id="8" value="" style="rounded=0;whiteSpace=wrap;html=1;fillColor=default;" vertex="1" parent="1">
                    <mxGeometry x="164" y="304" width="224" height="31" as="geometry"/>
                </mxCell>
                <mxCell id="9" value="" style="rounded=0;whiteSpace=wrap;html=1;fillColor=default;" vertex="1" parent="1">
                    <mxGeometry x="172" y="311.5" width="16" height="16" as="geometry"/>
                </mxCell>
                <mxCell id="10" value="" style="rounded=0;whiteSpace=wrap;html=1;fillColor=default;" vertex="1" parent="1">
                    <mxGeometry x="196" y="311.5" width="16" height="16" as="geometry"/>
                </mxCell>
                <mxCell id="11" value="" style="rounded=0;whiteSpace=wrap;html=1;fillColor=default;" vertex="1" parent="1">
                    <mxGeometry x="220" y="311.5" width="16" height="16" as="geometry"/>
                </mxCell>
                <mxCell id="12" value="" style="rounded=0;whiteSpace=wrap;html=1;fillColor=default;" vertex="1" parent="1">
                    <mxGeometry x="244" y="312" width="16" height="15.5" as="geometry"/>
                </mxCell>
                <mxCell id="13" value="" style="rounded=0;whiteSpace=wrap;html=1;fillColor=default;" vertex="1" parent="1">
                    <mxGeometry x="268" y="311.5" width="16" height="16" as="geometry"/>
                </mxCell>
                <mxCell id="14" value="" style="rounded=0;whiteSpace=wrap;html=1;fillColor=default;" vertex="1" parent="1">
                    <mxGeometry x="292" y="311.5" width="16" height="16" as="geometry"/>
                </mxCell>
                <mxCell id="15" value="" style="rounded=0;whiteSpace=wrap;html=1;fillColor=default;" vertex="1" parent="1">
                    <mxGeometry x="316" y="311.5" width="16" height="16" as="geometry"/>
                </mxCell>
                <mxCell id="16" value="" style="rounded=0;whiteSpace=wrap;html=1;fillColor=default;" vertex="1" parent="1">
                    <mxGeometry x="340" y="312" width="16" height="15.5" as="geometry"/>
                </mxCell>
                <mxCell id="17" value="" style="rounded=0;whiteSpace=wrap;html=1;fillColor=default;" vertex="1" parent="1">
                    <mxGeometry x="364" y="312" width="16" height="15.5" as="geometry"/>
                </mxCell>
                <mxCell id="18" value="" style="rounded=0;whiteSpace=wrap;html=1;fillColor=default;" vertex="1" parent="1">
                    <mxGeometry x="400" y="304" width="76" height="31" as="geometry"/>
                </mxCell>
                <mxCell id="19" value="" style="rounded=0;whiteSpace=wrap;html=1;fillColor=default;" vertex="1" parent="1">
                    <mxGeometry x="405" y="312.5" width="16" height="16" as="geometry"/>
                </mxCell>
                <mxCell id="20" value="" style="rounded=0;whiteSpace=wrap;html=1;fillColor=default;" vertex="1" parent="1">
                    <mxGeometry x="429" y="312.5" width="16" height="16" as="geometry"/>
                </mxCell>
                <mxCell id="21" value="" style="rounded=0;whiteSpace=wrap;html=1;fillColor=default;" vertex="1" parent="1">
                    <mxGeometry x="453" y="312.5" width="16" height="16" as="geometry"/>
                </mxCell>
                <mxCell id="23" value="Time" style="text;html=1;strokeColor=none;fillColor=none;align=center;verticalAlign=middle;whiteSpace=wrap;rounded=0;fontColor=#990000;" vertex="1" parent="1">
                    <mxGeometry x="460" y="356" width="60" height="30" as="geometry"/>
                </mxCell>
                <mxCell id="24" value="" style="endArrow=none;html=1;dashed=1;fontColor=#990000;exitX=1;exitY=1;exitDx=0;exitDy=0;strokeColor=#990000;" edge="1" parent="1" source="3">
                    <mxGeometry width="50" height="50" relative="1" as="geometry">
                        <mxPoint x="132" y="486" as="sourcePoint"/>
                        <mxPoint x="152" y="280" as="targetPoint"/>
                    </mxGeometry>
                </mxCell>
                <mxCell id="25" value="criteria met" style="text;html=1;strokeColor=none;fillColor=none;align=center;verticalAlign=middle;whiteSpace=wrap;rounded=0;fontColor=#990000;" vertex="1" parent="1">
                    <mxGeometry x="117" y="252" width="76" height="30" as="geometry"/>
                </mxCell>
                <mxCell id="26" value="criteria met" style="text;html=1;strokeColor=none;fillColor=none;align=center;verticalAlign=middle;whiteSpace=wrap;rounded=0;fontColor=#990000;" vertex="1" parent="1">
                    <mxGeometry x="350" y="252" width="76" height="30" as="geometry"/>
                </mxCell>
                <mxCell id="27" value="" style="endArrow=none;dashed=1;html=1;strokeWidth=1;strokeColor=#990000;fontColor=#990000;" edge="1" parent="1">
                    <mxGeometry width="50" height="50" relative="1" as="geometry">
                        <mxPoint x="389" y="336" as="sourcePoint"/>
                        <mxPoint x="388" y="276" as="targetPoint"/>
                    </mxGeometry>
                </mxCell>
                <mxCell id="29" value="criteria met" style="text;html=1;strokeColor=none;fillColor=none;align=center;verticalAlign=middle;whiteSpace=wrap;rounded=0;fontColor=#990000;" vertex="1" parent="1">
                    <mxGeometry x="438" y="252" width="76" height="30" as="geometry"/>
                </mxCell>
                <mxCell id="30" value="" style="endArrow=none;dashed=1;html=1;strokeWidth=1;strokeColor=#990000;fontColor=#990000;" edge="1" parent="1">
                    <mxGeometry width="50" height="50" relative="1" as="geometry">
                        <mxPoint x="477" y="336" as="sourcePoint"/>
                        <mxPoint x="476" y="276" as="targetPoint"/>
                    </mxGeometry>
                </mxCell>
            </root>
        </mxGraphModel>
    </diagram>
</mxfile>

================================================
File: tests/test_chat_room.py
================================================
""" test chat room """

import queue
import asyncio
import pytest

from whisperflow.chat_room import ChatRoom


async def listener_mock(queue_in: queue.Queue, stop_event: asyncio.Event):
    """collect items from queue"""
    while not stop_event.is_set():
        await asyncio.sleep(0.1)
        queue_in.put("hello")


async def processor_mock(queue_in, queue_out, stop_event):
    """collect items from queue"""
    while not stop_event.is_set():
        await asyncio.sleep(0.1)
        if not queue_in.empty():
            item = queue_in.get()
            queue_out.put(item)


async def speaker_mock(queue_in: queue.Queue, stop_event: asyncio.Event):
    """mock playing sound"""
    while not stop_event.is_set():
        await asyncio.sleep(0.1)
        if not queue_in.empty():
            item = queue_in.get()
            assert item is not None


@pytest.mark.asyncio
async def test_chat_room():
    """mock playing sound"""
    room = ChatRoom(listener_mock, speaker_mock, processor_mock)

    async def stop_chat():
        await asyncio.sleep(1)
        room.stop_chat()

    await asyncio.gather(room.start_chat(), stop_chat())
    assert room.stop_chat_event.is_set()


================================================
File: tests/test_streaming.py
================================================
""" test scenario module """

import asyncio
from queue import Queue

import pytest
import tests.utils as ut
import whisperflow.streaming as st
import whisperflow.fast_server as fs
import whisperflow.transcriber as ts


@pytest.mark.asyncio
async def test_simple():
    """test asyncio"""

    queue, should_stop = Queue(), [False]
    queue.put(1)

    async def dummy_transcriber(items: list) -> dict:
        await asyncio.sleep(0.1)
        if queue.qsize() == 0:
            should_stop[0] = True
        return {"text": str(len(items))}

    async def dummy_segment_closed(text: str) -> None:
        await asyncio.sleep(0.01)
        print(text)

    await st.transcribe(should_stop, queue, dummy_transcriber, dummy_segment_closed)
    assert queue.qsize() == 0


@pytest.mark.asyncio
async def test_transcribe_streaming(chunk_size=4096):
    """test streaming"""

    model = ts.get_model()
    queue, should_stop = Queue(), [False]
    res = ut.load_resource("3081-166546-0000")
    chunks = [
        res["audio"][i : i + chunk_size]
        for i in range(0, len(res["audio"]), chunk_size)
    ]

    async def dummy_transcriber(items: list) -> str:
        await asyncio.sleep(0.01)
        result = ts.transcribe_pcm_chunks(model, items)
        return result

    result = []

    async def dummy_segment_closed(text: str) -> None:
        await asyncio.sleep(0.01)
        result.append(text)

    task = asyncio.create_task(
        st.transcribe(should_stop, queue, dummy_transcriber, dummy_segment_closed)
    )

    for chunk in chunks:
        queue.put(chunk)
        await asyncio.sleep(0.01)

    await asyncio.sleep(1)
    should_stop[0] = True
    await task

    assert len(result) > 0


def test_streaming():
    """test hugging face image generation"""
    queue = Queue()
    queue.put(1)
    queue.put(2)
    res = st.get_all(queue)
    assert res == [1, 2]

    res = st.get_all(None)
    assert not res


@pytest.mark.asyncio
@pytest.mark.timeout(60)
async def test_ws(chunk_size=4096):
    """test health api"""
    client = ut.TestClient(fs.app)
    with client.websocket_connect("/ws") as websocket:
        res = ut.load_resource("3081-166546-0000")
        chunks = [
            res["audio"][i : i + chunk_size]
            for i in range(0, len(res["audio"]), chunk_size)
        ]

        for chunk in chunks:
            websocket.send_bytes(chunk)

        await asyncio.sleep(3)
        websocket.close()

    assert client


================================================
File: tests/test_transcriber.py
================================================
""" test transcriber """

import pytest
from jiwer import wer
import tests.utils as ut

import whisperflow.fast_server as fr
import whisperflow.transcriber as tr


def test_load_model():
    """test load model from disl"""
    model = tr.get_model()
    assert model is not None

    resource = ut.load_resource("3081-166546-0000")

    result = tr.transcribe_pcm_chunks(model, [resource["audio"]])
    expected = resource["expected"]["final_ground_truth"]

    error = wer(result["text"].lower(), expected.lower())
    assert error < 0.1


def test_transcribe_chunk():
    """test transcribe pcm chunk"""
    resource = ut.load_resource("3081-166546-0000")
    client = ut.TestClient(fr.app)
    response = client.get("/health")
    assert response.status_code == 200

    path = ut.get_resource_path("3081-166546-0000", "wav")
    with open(path, "br") as file:
        response = client.post(
            url="/transcribe_pcm_chunk",
            data={"model_name": "tiny.en.pt"},
            files=[("files", file)],
        )

    assert response.status_code == 200

    expected = resource["expected"]["final_ground_truth"]
    error = wer(response.json()["text"].lower(), expected.lower())
    assert error < 0.1


@pytest.mark.asyncio
async def test_transcribe_chunk_async():
    """test transcribe async"""
    model = tr.get_model()
    assert model is not None
    resource = ut.load_resource("3081-166546-0000")
    result = await tr.transcribe_pcm_chunks_async(model, [resource["audio"]])
    expected = resource["expected"]["final_ground_truth"]
    error = wer(result["text"].lower(), expected.lower())
    assert error < 0.1


================================================
File: tests/utils.py
================================================
""" test utils class """

import os
import json
from starlette.testclient import TestClient
import whisperflow.fast_server as fs


def get_resource_path(name: str, extension: str) -> str:
    "get resources path"
    current_path = os.path.dirname(__file__)
    path = os.path.join(current_path, f"./resources/{name}")
    return f"{path}.{extension}"


def load_resource(name: str) -> dict:
    "load resource"
    result = {}

    with open(get_resource_path(name, "wav"), "br") as file:
        result["audio"] = file.read()

    with open(get_resource_path(name, "json"), "r", encoding="utf-8") as file:
        result["expected"] = json.load(file)

    return result


def test_fast_api():
    """test health api"""
    with TestClient(fs.app) as client:
        response = client.get("/health")
        assert response.status_code == 200 and bool(response.text)


================================================
File: tests/audio/test_audio.py
================================================
""" test chat room """

import queue
import asyncio
import pytest


from whisperflow.audio.microphone import capture_audio


@pytest.mark.asyncio
async def test_capture_mic():
    """test capturing microphone"""
    stop_event = asyncio.Event()
    audio_chunks = queue.Queue()

    async def stop_capturing():
        await asyncio.sleep(0.1)
        stop_event.set()

    await asyncio.gather(capture_audio(audio_chunks, stop_event), stop_capturing())
    assert stop_event.is_set()
    assert not audio_chunks.empty()


================================================
File: tests/benchmark/test_benchmark.py
================================================
"""benchamrk"""

import json
import time
import pandas as pd

import requests
import jiwer as jw
import websocket as ws
import tests.utils as ut


def test_health(url="http://localhost:8181/health"):
    """basic test"""
    result = requests.get(url=url, timeout=1)
    assert result.status_code == 200


def get_res(websocket):
    """try read with timout"""
    try:
        result = json.loads(websocket.recv())
        print_result(result)
        return result
    except ws.WebSocketTimeoutException:
        return {}


def print_result(result: dict):
    """print result and execution time"""
    print(result["is_partial"], round(result["time"], 2), result["data"]["text"])


def test_send_chunks(url="ws://localhost:8181/ws", chunk_size=4096):
    """send chunks"""
    websocket = ws.create_connection(url)
    websocket.settimeout(0.1)

    resource = ut.load_resource("3081-166546-0000")
    chunks = [
        resource["audio"][i : i + chunk_size]
        for i in range(0, len(resource["audio"]), chunk_size)
    ]

    df_result = pd.DataFrame(columns=["is_partial", "latency", "result"])
    for chunk in chunks:
        websocket.send_bytes(chunk)
        res = get_res(websocket)
        if res:
            df_result.loc[len(df_result)] = [
                res["is_partial"],
                round(res["time"], 2),
                res["data"]["text"],
            ]

    attempts = 0
    while attempts < 3:
        res = get_res(websocket)
        if res:
            attempts = 0
            df_result.loc[len(df_result)] = [
                res["is_partial"],
                round(res["time"], 2),
                res["data"]["text"],
            ]
        else:
            attempts += 1
            time.sleep(1)

    pd.set_option("max_colwidth", 800)
    # print(df_result.to_string(justify='left', index=False))
    print("Latency Stats:\n", df_result["latency"].describe())

    actual = df_result.loc[len(df_result) - 1]["result"].lower().strip()
    expected = resource["expected"]["final_ground_truth"].lower().strip()

    error = round(jw.wer(actual, expected), 2)
    assert error < 0.1
    websocket.close()


if __name__ == "__main__":
    print("Starting Whisper-Flow Benchmark")
    test_send_chunks()
    print("Whisper-Flow Benchmark Completed")


================================================
File: tests/examples/mic_transcribe.py
================================================
"""
a test app that streams
audio  from the mic to whisper flow
requires pip install PyAudio
"""

import json
import asyncio
import pyaudio
import websockets


async def start_transcription(url="ws://0.0.0.0:8181/ws"):
    """stream mic audio to server"""
    async with websockets.connect(url) as websocket:
        result = []
        await asyncio.gather(
            capture_audio(websocket, result), receive_transcription(websocket, result)
        )
        print(f"* done recording, collecting data")
        print("Colllected text is \n", " ".join(result))


async def capture_audio(websocket: websockets.WebSocketClientProtocol, result: list):
    """capture the mic stream"""
    chunk, rate, record_sec = 1024, 16000, 30
    p = pyaudio.PyAudio()
    stream = p.open(
        format=pyaudio.paInt16,
        channels=1,
        rate=rate,
        input=True,
        frames_per_buffer=chunk,
    )
    print("* recording")

    for _ in range(0, int(rate / chunk * record_sec)):
        data = stream.read(chunk)
        await websocket.send(data)
        await asyncio.sleep(0.01)

    stream.close()
    p.terminate()


async def receive_transcription(websocket, result: list):
    """print transcription"""
    while True:
        try:
            await asyncio.sleep(0.01)
            tmp = json.loads(await websocket.recv())
            if not tmp["is_partial"]:
                result.append(tmp["data"]["text"])
            print(tmp["is_partial"], round(tmp["time"], 2), tmp["data"]["text"])
        except Exception:
            print("No transcription available")


asyncio.run(start_transcription())


================================================
File: tests/resources/3081-166546-0000.json
================================================
{
    "partial_ground_truths":
    [
        "wantoha",
        "When we took",
        "When we took our",
        "When we took our seat",
        "When we took our seats",
        "When we took our seats",
        "When we took our seats at the",
        "When we took our seats at the Brookway",
        "When we took our seats at the breakfast",
        "When we took our seats at the breakfast table",
        "When we took our seats at the breakfast table,",
        "When we took our seats at the breakfast table",
        "When we took our seats at the breakfast table, it went",
        "When we took our seats at the breakfast table, it was one",
        "When we took our seats at the breakfast table, it was with",
        "When we took our seats at the breakfast table it was with the",
        "When we took our seats at the breakfast table, it was with the feeling",
        "When we took our seats at the breakfast table, it was with the feeling of",
        "When we took our seats at the breakfast table, it was with the feeling of",
        "When we took our seats at the breakfast table, it was with the feeling of being",
        "When we took our seats at the breakfast table, it was with the feeling of being no one",
        "When we took our seats at the breakfast table, it was with the feeling of being no longer",
        "When we took our seats at the breakfast table, it was with the feeling of being no longer",
        "When we took our seats at the breakfast table, it was with the feeling of being no longer looked away",
        "When we took our seats at the breakfast table, it was with the feeling of being no longer looked upon",
        "When we took our seats at the breakfast table, it was with the feeling of being no longer looked upon",
        "When we took our seats at the breakfast table, it was with the feeling of being no longer looked upon as",
        "When we took our seats at the breakfast table, it was with the feeling of being no longer looked upon as connected",
        "When we took our seats at the breakfast table, it was with the feeling of being no longer looked upon as connected",
        "When we took our seats at the breakfast table, it was with the feeling of being no longer looked upon as connected",
        "When we took our seats at the breakfast table, it was with the feeling of being no longer looked upon as connected in any way",
        "When we took our seats at the breakfast table, it was with the feeling of being no longer looked upon as connected in any way",
        "When we took our seats at the breakfast table, it was with the feeling of being no longer looked upon as connected in any way",
        "When we took our seats at the breakfast table, it was with the feeling of being no longer looked upon as connected in any way with the",
        "When we took our seats at the breakfast table, it was with the feeling of being no longer looked upon as connected in any way with this",
        "When we took our seats at the breakfast table, it was with the feeling of being no longer looked upon as connected in any way with this case",
        "When we took our seats at the breakfast table, it was with the feeling of being no longer looked upon as connected in any way with this case",
        "When we took our seats at the breakfast table, it was with the feeling of being no longer looked upon as connected in any way with this case",
        "When we took our seats at the breakfast table, it was with the feeling of being no longer looked upon as connected in any way with this case",
        "When we took our seats at the breakfast table, it was with the feeling of being no longer looked upon as connected in any way with this case",
        "When we took our seats at the breakfast table, it was with the feeling of being no longer looked upon as connected in any way with this case",
        "When we took our seats at the breakfast table, it was with the feeling of being no longer looked upon as connected in any way with this case"
    ],
    "final_ground_truth": "WHEN WE TOOK OUR SEATS AT THE BREAKFAST TABLE IT WAS WITH THE FEELING OF BEING NO LONGER LOOKED UPON AS CONNECTED IN ANY WAY WITH THIS CASE"
}

================================================
File: whisperflow/__init__.py
================================================
""" add package version """

__version__ = "1.0.0"


================================================
File: whisperflow/chat_room.py
================================================
"""
Implements conversation loop: capture
audio -> speech to text -> custom action -> text to speech -> play audio
"""

import queue
import asyncio


class ChatRoom:
    """
    A class enabling real-time communication with microphone input and speaker output.
    It supports speech-to-text (STT) and text-to-speech (TTS)
    processing, with an optional handler for custom text analysis.
    """

    def __init__(self, listener, speaker, processor):
        self.audio_chunks = queue.Queue()
        self.text_result = queue.Queue()
        self.listener = listener
        self.speaker = speaker
        self.processor = processor
        self.stop_chat_event = asyncio.Event()

    async def start_chat(self):
        """start chat by listening to mic"""
        self.stop_chat_event.clear()

        # start listener and processor
        await asyncio.gather(
            self.listener(self.audio_chunks, self.stop_chat_event),
            self.processor(self.audio_chunks, self.text_result, self.stop_chat_event),
            self.speaker(self.text_result, self.stop_chat_event),
        )

    def stop_chat(self):
        """stop chat and release resources"""
        self.stop_chat_event.set()
        assert self.stop_chat_event.is_set()


================================================
File: whisperflow/fast_server.py
================================================
""" fast api declaration """

import logging
from typing import List
from fastapi import FastAPI, WebSocket, Form, File, UploadFile

from whisperflow import __version__
import whisperflow.streaming as st
import whisperflow.transcriber as ts


app = FastAPI()
sessions = {}


@app.get("/health", response_model=str)
def health():
    """health function on API"""
    return f"Whisper Flow V{__version__}"


@app.post("/transcribe_pcm_chunk", response_model=dict)
def transcribe_pcm_chunk(
    model_name: str = Form(...), files: List[UploadFile] = File(...)
):
    """transcribe chunk"""
    model = ts.get_model(model_name)
    content = files[0].file.read()
    return ts.transcribe_pcm_chunks(model, [content])


@app.websocket("/ws")
async def websocket_endpoint(websocket: WebSocket):
    """webscoket implementation"""
    model = ts.get_model()

    async def transcribe_async(chunks: list):
        return await ts.transcribe_pcm_chunks_async(model, chunks)

    async def send_back_async(data: dict):
        await websocket.send_json(data)

    try:
        await websocket.accept()
        session = st.TranscribeSession(transcribe_async, send_back_async)
        sessions[session.id] = session

        while True:
            data = await websocket.receive_bytes()
            session.add_chunk(data)
    except Exception as exception:  # pylint: disable=broad-except
        logging.error(exception)
        await session.stop()
        await websocket.close()


================================================
File: whisperflow/streaming.py
================================================
""" test scenario module """

import time
import uuid
import asyncio
from queue import Queue
from typing import Callable


def get_all(queue: Queue) -> list:
    """get_all from queue"""
    res = []
    while queue and not queue.empty():
        res.append(queue.get())
    return res


async def transcribe(
    should_stop: list,
    queue: Queue,
    transcriber: Callable[[list], str],
    segment_closed: Callable[[dict], None],
):
    """the transcription loop"""
    window, prev_result, cycles = [], {}, 0

    while not should_stop[0]:
        start = time.time()
        await asyncio.sleep(0.01)
        window.extend(get_all(queue))

        if not window:
            continue

        result = {
            "is_partial": True,
            "data": await transcriber(window),
            "time": (time.time() - start) * 1000,
        }

        if should_close_segment(result, prev_result, cycles):
            window, prev_result, cycles = [], {}, 0
            result["is_partial"] = False
        elif result["data"]["text"] == prev_result.get("data", {}).get("text", ""):
            cycles += 1
        else:
            cycles = 0
            prev_result = result

        if result["data"]["text"]:
            await segment_closed(result)


def should_close_segment(result: dict, prev_result: dict, cycles, max_cycles=1):
    """return if segment should be closed"""
    return cycles >= max_cycles and result["data"]["text"] == prev_result.get(
        "data", {}
    ).get("text", "")


class TranscribeSession:  # pylint: disable=too-few-public-methods
    """transcription state"""

    def __init__(self, transcribe_async, send_back_async) -> None:
        """ctor"""
        self.id = uuid.uuid4()  # pylint: disable=invalid-name
        self.queue = Queue()
        self.should_stop = [False]
        self.task = asyncio.create_task(
            transcribe(self.should_stop, self.queue, transcribe_async, send_back_async)
        )

    def add_chunk(self, chunk: bytes):
        """add new chunk"""
        self.queue.put_nowait(chunk)

    async def stop(self):
        """stop session"""
        self.should_stop[0] = True
        await self.task


================================================
File: whisperflow/transcriber.py
================================================
""" transcriber """

import os
import asyncio

import torch
import numpy as np

import whisper
from whisper import Whisper


models = {}


def get_model(file_name="tiny.en.pt") -> Whisper:
    """load models from disk"""
    if file_name not in models:
        path = os.path.join(os.path.dirname(__file__), f"./models/{file_name}")
        models[file_name] = whisper.load_model(path).to(
            "cuda" if torch.cuda.is_available() else "cpu"
        )
    return models[file_name]


def transcribe_pcm_chunks(
    model: Whisper, chunks: list, lang="en", temperature=0.1, log_prob=-0.5
) -> dict:
    """transcribes pcm chunks list"""
    arr = (
        np.frombuffer(b"".join(chunks), np.int16).flatten().astype(np.float32) / 32768.0
    )
    return model.transcribe(
        arr,
        fp16=False,
        language=lang,
        logprob_threshold=log_prob,
        temperature=temperature,
    )


async def transcribe_pcm_chunks_async(
    model: Whisper, chunks: list, lang="en", temperature=0.1, log_prob=-0.5
) -> dict:
    """transcribes pcm chunks async"""
    return await asyncio.get_running_loop().run_in_executor(
        None, transcribe_pcm_chunks, model, chunks, lang, temperature, log_prob
    )


================================================
File: whisperflow/audio/microphone.py
================================================
"""
capture audio from microphone
"""

import queue
import asyncio
import pyaudio


async def capture_audio(queue_chunks: queue.Queue, stop_event: asyncio.Event):
    """capture the mic stream"""
    chunk, rate, record_sec = 1024, 16000, 1
    audio = pyaudio.PyAudio()
    stream = audio.open(
        format=pyaudio.paInt16,
        channels=1,
        rate=rate,
        input=True,
        frames_per_buffer=chunk,
    )

    while not stop_event.is_set():
        for _ in range(0, int(rate / chunk * record_sec)):
            data = stream.read(chunk)
            queue_chunks.put(data)
            await asyncio.sleep(0.01)

    stream.close()
    audio.terminate()


================================================
File: .devcontainer/devcontainer.json
================================================
// For format details, see https://aka.ms/devcontainer.json. For config options, see the
// README at: https://github.com/devcontainers/templates/tree/main/src/python
{
	"name": "Python 3",
	// Or use a Dockerfile or Docker Compose file. More info: https://containers.dev/guide/dockerfile
	"image": "mcr.microsoft.com/devcontainers/python:1-3.10-bullseye"

	// Features to add to the dev container. More info: https://containers.dev/features.
	// "features": {},

	// Use 'forwardPorts' to make a list of ports inside the container available locally.
	// "forwardPorts": [],

	// Use 'postCreateCommand' to run commands after the container is created.
	// "postCreateCommand": "pip3 install --user -r requirements.txt",

	// Configure tool-specific properties.
	// "customizations": {},

	// Uncomment to connect as root instead. More info: https://aka.ms/dev-containers-non-root.
	// "remoteUser": "root"
}


================================================
File: .github/dependabot.yml
================================================
# To get started with Dependabot version updates, you'll need to specify which
# package ecosystems to update and where the package manifests are located.
# Please see the documentation for more information:
# https://docs.github.com/github/administering-a-repository/configuration-options-for-dependency-updates
# https://containers.dev/guide/dependabot

version: 2
updates:
 - package-ecosystem: "devcontainers"
   directory: "/"
   schedule:
     interval: weekly


================================================
File: .github/workflows/docker-image.yml
================================================
name: Docker Image CI

on:
  push:
    branches: [ "main" ]
  pull_request:
    branches: [ "main" ]

jobs:

  build:

    runs-on: ubuntu-latest

    steps:
    - uses: actions/checkout@v3
    - name: Build the Docker image
      run: docker build . --file Dockerfile.test --tag whisperflow-docker:$(date +%s)
